---
title: "Final Project STAT206"
author: "Anshara Fatima"
date: "2025-07-01"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Clear environment
rm(list = ls())
```

```{r}
# Step1: Load Dataset 

my_data <- read.csv("air_quality_health_dataset.csv", stringsAsFactors = FALSE)

# Preview the data
head(my_data)
```

```{r}
#Step 2: Handling Missing Data

# Load library
library(naniar) #naniar package helps find, explore and fix missing data eg. N/A values 

# Summary of missing values
colSums(is.na(my_data)) #tells us how many missing values are there in each column after turning all N/As into TRUE

# Visualize missing data
gg_miss_var(my_data) #naniar package function. shows a bar graph that displays how much data is missing in each column so it is easier to see

# Impute missing numeric values with mean
# Looks at every column one by one. If it is a number column, it finds average of that column while ignoring missin values, then it fills missing values with that average of each column respectively. 
my_data[] <- lapply(my_data, function(x) {
  if (is.numeric(x)) {
    x[is.na(x)] <- mean(x, na.rm = TRUE)
  }
  return(x)
})

```
```{r}
# This code will find outliers in my data and will replace them with a safe limit
# Step 3: Function to cap outliers using IQR

#  The following function creates a custom function called cap_outliers. It will work on one column at a time (x).
cap_outliers <- function(x) {
  if (is.numeric(x)) {                    #if the column is in numbers, do:
    Q1 <- quantile(x, 0.25, na.rm = TRUE) # no. below with 25% data lies (low                                                 quartile)
    Q3 <- quantile(x, 0.75, na.rm = TRUE) # no. below which 75% data lies (upper                                              quartile)
    IQR_val <- Q3 - Q1   # difference between Q1 & Q3 telling us how far spread out                            middle values are
    lower <- Q1 - 1.5 * IQR_val # Lower limit: Anything smaller than this is an                                      outlier.
    upper <- Q3 + 1.5 * IQR_val # Upper limit: Anything bigger than this is an                                       outlier.
    x[x < lower] <- lower       # if a number is too small, change to lower limit
    x[x > upper] <- upper       # if a number is too big, change to upper limit
  }
  return(x)
}

# Apply to numeric columns, each column one by one
my_data[] <- lapply(my_data, cap_outliers)

```

```{r}

# Step 4: Create new column; percentage of hospital capacity used by respiratory admissions

#This new column tells me what % of the hospital beds are being used by patients

my_data$resp_admission_pct <- (my_data$hospital_admissions / my_data$hospital_capacity) * 100

# View the new column (First few results)
head(my_data$resp_admission_pct)

# Add to the data preview; lets me see my data in a table 
head(my_data[, c("city", "hospital_admissions", "hospital_capacity", "resp_admission_pct")])

```
```{r}
# Step 5: Subset Data for Each City
cities <- unique(my_data$city) # looks at city column in my data and finds 
                               # all city names & removes duplicates 
for (c in cities) {       # this starts a loop. for every city, do the same:
  city_data <- subset(my_data, city == c)  #pick all data for this one city
  write.csv(city_data, paste0(c, ".csv"), row.names = FALSE) #saves .csv file for                                                  city by naming it after each city
}

```

```{r}
# Step 6: List city files

# List all CSV files in the directory
city_files <- list.files(pattern = "\\.csv$", ignore.case = TRUE)

# Keep only safe file names with regular letters, numbers, space, underscore, dot, or dash
clean_city_files <- city_files[grepl("^[A-Za-z0-9 _.-]+\\.csv$", city_files)]

# Check which files are valid
print(clean_city_files)

# Open each .csv file one by one and it shows me a small part of it so I can check the data
for (file in clean_city_files) {
  city_data <- read.csv(file)
  cat("Preview of:", file, "\n")
  print(head(city_data[, 1:6]))
}
```


```{r}
# Step 7: Combine all the data into one file and write down a function that will help calculate five values summaries (min, max, mean, median, SD) for all the continuous variables present in the data. Use this function to calculate 5 values summary for each continuous variable and report your results in the form of a table (generated by your function) and also report it in your RMarkdown report.

# Load necessary packages
library(dplyr)
library(knitr)

# List all .csv files
city_files <- list.files(pattern = "\\.csv$", ignore.case = TRUE)

# Keep only file names with safe characters (A–Z, a–z, 0–9, underscore, space, dot, dash)
city_files <- city_files[grepl("^[A-Za-z0-9 _.-]+\\.csv$", city_files)]

# Now safely read the cleaned files
library(dplyr)

city_data_list <- lapply(city_files, function(file) {
  df <- read.csv(file)
  # Select only consistent columns from each file
  select(df, city, aqi, pm2_5, pm10, no2, o3, temperature, humidity,
         hospital_admissions, hospital_capacity)
})

# Combine all cleaned city data into one dataset
combined_data <- bind_rows(city_data_list)


# Combine the list into one data frame
combined_data <- bind_rows(city_data_list)

# Function for five-number summary + SD: This function creates a simple summary table of important numbers (min, max, mean, median, SD) for every numeric column in my data

five_summary <- function(df) {
  numeric_df <- df %>% select(where(is.numeric))
  
  summary_df <- data.frame(
    Variable = names(numeric_df),
    Min = sapply(numeric_df, min, na.rm = TRUE),
    Max = sapply(numeric_df, max, na.rm = TRUE),
    Mean = sapply(numeric_df, mean, na.rm = TRUE),
    Median = sapply(numeric_df, median, na.rm = TRUE),
    SD = sapply(numeric_df, sd, na.rm = TRUE)
  )
  
  return(summary_df)
}

# Run the summary function
summary_table <- five_summary(combined_data)

# Display the results
kable(summary_table, caption = "Five-Number Summary (plus SD) of Continuous Variables")

# This is to save combined_data (after combining all cities)
write.csv(combined_data, "combined_data.csv", row.names = FALSE)

```

```{r}
# Step 8: Which city has the greatest number of cases reported?
library(dplyr)

# Group by city and sum hospital admissions
city_cases <- combined_data %>%
  group_by(city) %>%
  summarise(total_admissions = sum(hospital_admissions, na.rm = TRUE)) %>%
  arrange(desc(total_admissions))

# Display the full table
print(city_cases)

# Show the city with the greatest number of cases
top_city <- city_cases[1, ]
cat("City with the greatest number of hospital admissions:", 
    top_city$city, "with", top_city$total_admissions, "cases.")
```
```{r}
# Step 9: Compare the temperatures of rural, suburban and urban areas

# Add area_type column manually
combined_data$area_type <- case_when(
  combined_data$city %in% c("Los Angeles", "Beijing", "Tokyo", "Cairo") ~ "Urban",
  combined_data$city %in% c("London", "São Paulo") ~ "Suburban",
  combined_data$city %in% c("Mexico City", "Delhi") ~ "Rural",
  TRUE ~ "Urban"  # default
)

library(dplyr)
library(ggplot2)

# Check if 'area_type' column exists
if("area_type" %in% colnames(combined_data)) {
  
  # Summary statistics of temperature by area type
  temp_summary <- combined_data %>%
    group_by(area_type) %>%
    summarise(
      Mean_Temperature = mean(temperature, na.rm = TRUE),
      Median_Temperature = median(temperature, na.rm = TRUE),
      SD_Temperature = sd(temperature, na.rm = TRUE),
      .groups = "drop"
    )
  
  print(temp_summary)

  # Visualize the comparison
  ggplot(combined_data, aes(x = area_type, y = temperature, fill = area_type)) +
    geom_boxplot() +
    labs(title = "Temperature Comparison by Area Type",
         x = "Area Type",
         y = "Temperature") +
    theme_minimal()
  
} else {
  cat("The dataset does not have an 'area_type' column. Please add it before proceeding.")
}

```


```{r}
# Do urban areas have higher PM2.5 and PM10 compared to suburban and rural areas?
# show this comparison using a bar chart for each city.

# Load necessary packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for data visualization
library(tidyr)     # for reshaping data

# ----------------------------
# Add area type to each city
# ----------------------------
# We will classify cities manually into Urban, Suburban, and Rural
combined_data$area_type <- case_when(
  combined_data$city %in% c("Delhi", "Beijing", "Tokyo", "Cairo") ~ "Urban",
  combined_data$city %in% c("London", "São Paulo") ~ "Suburban",
  combined_data$city %in% c("Mexico City", "Los Angeles") ~ "Rural",
  TRUE ~ "Urban"  # any remaining cities will default to Urban
)

# ----------------------------
# Calculate average PM2.5 and PM10 for each city and area type
# ----------------------------
pm_summary <- combined_data %>%
  group_by(city, area_type) %>%
  summarise(
    avg_pm2_5 = mean(pm2_5, na.rm = TRUE),   # average PM2.5
    avg_pm10 = mean(pm10, na.rm = TRUE),     # average PM10
    .groups = "drop"
  )

# ----------------------------
# Reshape the data so both PM2.5 and PM10 can be plotted together
# ----------------------------
# This turns two columns (avg_pm2_5 and avg_pm10) into one column of values
pm_long <- pm_summary %>%
  pivot_longer(
    cols = c(avg_pm2_5, avg_pm10),
    names_to = "pollutant",      # this new column will say "avg_pm2_5" or "avg_pm10"
    values_to = "value"          # this will contain the numeric values
  )

# ----------------------------
# Create bar plot to compare pollution levels by area type for each city
# ----------------------------
ggplot(pm_long, aes(x = city, y = value, fill = area_type)) +
  geom_bar(stat = "identity", position = "dodge") +  # make side-by-side bars
  facet_wrap(~ pollutant, scales = "free_y") +       # separate plot for PM2.5 and PM10
  labs(
    title = "Comparison of PM2.5 and PM10 by City and Area Type",
    x = "City",
    y = "Average Value",
    fill = "Area Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # rotate city names for readability

```

```{r}
# Step 11: Is there any correlation between the aqi,PM 2.5, PM 10, no2, O3, temperature and humidity in our dataset, show it using a correlation heatmap.
# Load required packages
library(ggplot2)
library(reshape2)   # for melting the correlation matrix
library(corrplot)   # for heatmap 

# ----------------------------
# Select only relevant numeric columns
# ----------------------------
# We will only include the variables mentioned in the question
corr_data <- combined_data %>%
  select(aqi, pm2_5, pm10, no2, o3, temperature, humidity)

# ----------------------------
# Calculate correlation matrix
# ----------------------------
# This shows how strongly each variable is related to the others
cor_matrix <- cor(corr_data, use = "complete.obs")

# ----------------------------
# Melt the matrix into long format for plotting with ggplot
# ----------------------------
# Melting turns my big table of correlations into a simple list of variable pairs 
#and their values. This makes it easier to plot a heatmap to show correlation. 
cor_long <- melt(cor_matrix)

# ----------------------------
# Plot heatmap using ggplot2
# ----------------------------
ggplot(cor_long, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +  # add white grid lines
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name = "Correlation") +
  labs(title = "Correlation Heatmap of Air Quality & Environmental Variables",
       x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))


```



