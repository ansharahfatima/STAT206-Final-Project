---
title: "Final Project STAT206"
author: "Anshara Fatima"
date: "2025-07-01"
output:
  pdf_document: default
  html_document: default
---

```{r}
# Clear environment
rm(list = ls())
```

```{r}
# Step1: Load Dataset 

my_data <- read.csv("air_quality_health_dataset.csv", stringsAsFactors = FALSE)

# Preview the data
head(my_data)
```

```{r}
#Step 2: Handling Missing Data

# Load library
library(naniar)

# Summary of missing values
colSums(is.na(my_data))

# Visualize missing data
gg_miss_var(my_data)

# Impute missing numeric values with mean
my_data[] <- lapply(my_data, function(x) {
  if (is.numeric(x)) {
    x[is.na(x)] <- mean(x, na.rm = TRUE)
  }
  return(x)
})

```
```{r}
# Step 3: Function to cap outliers using IQR
cap_outliers <- function(x) {
  if (is.numeric(x)) {
    Q1 <- quantile(x, 0.25, na.rm = TRUE)
    Q3 <- quantile(x, 0.75, na.rm = TRUE)
    IQR_val <- Q3 - Q1
    lower <- Q1 - 1.5 * IQR_val
    upper <- Q3 + 1.5 * IQR_val
    x[x < lower] <- lower
    x[x > upper] <- upper
  }
  return(x)
}

# Apply to numeric columns
my_data[] <- lapply(my_data, cap_outliers)

```

```{r}
# Step 4: Create new column; percentage of hospital capacity used by respiratory admissions
my_data$resp_admission_pct <- (my_data$hospital_admissions / my_data$hospital_capacity) * 100

# View the new column
head(my_data$resp_admission_pct)

# Add to the data preview
head(my_data[, c("city", "hospital_admissions", "hospital_capacity", "resp_admission_pct")])

```
```{r}
# Step 5: Subset Data for Each City
cities <- unique(my_data$city)

for (c in cities) {
  city_data <- subset(my_data, city == c)
  write.csv(city_data, paste0(c, ".csv"), row.names = FALSE)
}

```

```{r}
# Step 6: List city files

# List all CSV files in the directory
city_files <- list.files(pattern = "\\.csv$", ignore.case = TRUE)

# Keep only safe file names with regular letters, numbers, space, underscore, dot, or dash
clean_city_files <- city_files[grepl("^[A-Za-z0-9 _.-]+\\.csv$", city_files)]

# Check which files are valid
print(clean_city_files)

# Try reading them now
for (file in clean_city_files) {
  city_data <- read.csv(file)
  cat("Preview of:", file, "\n")
  print(head(city_data[, 1:6]))
}
```


```{r}
# Step 7: Combine all the data into one file and write down a function that will help calculate five values summaries (min, max, mean, median, SD) for all the continuous variables present in the data. Use this function to calculate 5 values summary for each continuous variable and report your results in the form of a table (generated by your function) and also report it in your RMarkdown report.

# Load necessary packages
library(dplyr)
library(knitr)

# 1: List all city CSV files
city_files <- list.files(pattern = ".csv")

# Step 1: List all .csv files
city_files <- list.files(pattern = "\\.csv$", ignore.case = TRUE)

# Step 2: Keep only file names with safe characters (A–Z, a–z, 0–9, underscore, space, dot, dash)
city_files <- city_files[grepl("^[A-Za-z0-9 _.-]+\\.csv$", city_files)]

# Step 3: Now safely read the cleaned files
library(dplyr)

city_data_list <- lapply(city_files, function(file) {
  df <- read.csv(file)
  # Select only consistent columns from each file
  select(df, city, aqi, pm2_5, pm10, no2, o3, temperature, humidity,
         hospital_admissions, hospital_capacity)
})

# Combine all cleaned city data into one dataset
combined_data <- bind_rows(city_data_list)


# 3: Combine the list into one data frame
combined_data <- bind_rows(city_data_list)

# 4: Function for five-number summary + SD
five_summary <- function(df) {
  numeric_df <- df %>% select(where(is.numeric))
  
  summary_df <- data.frame(
    Variable = names(numeric_df),
    Min = sapply(numeric_df, min, na.rm = TRUE),
    Max = sapply(numeric_df, max, na.rm = TRUE),
    Mean = sapply(numeric_df, mean, na.rm = TRUE),
    Median = sapply(numeric_df, median, na.rm = TRUE),
    SD = sapply(numeric_df, sd, na.rm = TRUE)
  )
  
  return(summary_df)
}

# 5: Run the summary function
summary_table <- five_summary(combined_data)

# 6: Display the results
kable(summary_table, caption = "Five-Number Summary (plus SD) of Continuous Variables")

# Put this at the end to save combined_data (after combining all cities)
write.csv(combined_data, "combined_data.csv", row.names = FALSE)

```

```{r}
# Step 8: Which city has the greatest number of cases reported?
library(dplyr)

# Group by city and sum hospital admissions
city_cases <- combined_data %>%
  group_by(city) %>%
  summarise(total_admissions = sum(hospital_admissions, na.rm = TRUE)) %>%
  arrange(desc(total_admissions))

# Display the full table
print(city_cases)

# Show the city with the greatest number of cases
top_city <- city_cases[1, ]
cat("City with the greatest number of hospital admissions:", 
    top_city$city, "with", top_city$total_admissions, "cases.")
```
```{r}
# Step 9: Compare the temperatures of rural, suburban and urban areas

# Add area_type column manually
combined_data$area_type <- case_when(
  combined_data$city %in% c("Los Angeles", "Beijing", "Tokyo", "Cairo") ~ "Urban",
  combined_data$city %in% c("London", "São Paulo") ~ "Suburban",
  combined_data$city %in% c("Mexico City", "Delhi") ~ "Rural",
  TRUE ~ "Urban"  # default
)

library(dplyr)
library(ggplot2)

# Check if 'area_type' column exists
if("area_type" %in% colnames(combined_data)) {
  
  # Summary statistics of temperature by area type
  temp_summary <- combined_data %>%
    group_by(area_type) %>%
    summarise(
      Mean_Temperature = mean(temperature, na.rm = TRUE),
      Median_Temperature = median(temperature, na.rm = TRUE),
      SD_Temperature = sd(temperature, na.rm = TRUE),
      .groups = "drop"
    )
  
  print(temp_summary)

  # Visualize the comparison
  ggplot(combined_data, aes(x = area_type, y = temperature, fill = area_type)) +
    geom_boxplot() +
    labs(title = "Temperature Comparison by Area Type",
         x = "Area Type",
         y = "Temperature") +
    theme_minimal()
  
} else {
  cat("The dataset does not have an 'area_type' column. Please add it before proceeding.")
}

```


```{r}
# Do urban areas have higher PM2.5 and PM10 compared to suburban and rural areas?
# show this comparison using a bar chart for each city.

# Load necessary packages
library(dplyr)     # for data manipulation
library(ggplot2)   # for data visualization
library(tidyr)     # for reshaping data

# ----------------------------
# Add area type to each city
# ----------------------------
# We will classify cities manually into Urban, Suburban, and Rural
combined_data$area_type <- case_when(
  combined_data$city %in% c("Delhi", "Beijing", "Tokyo", "Cairo") ~ "Urban",
  combined_data$city %in% c("London", "São Paulo") ~ "Suburban",
  combined_data$city %in% c("Mexico City", "Los Angeles") ~ "Rural",
  TRUE ~ "Urban"  # any remaining cities will default to Urban
)

# ----------------------------
# Calculate average PM2.5 and PM10 for each city and area type
# ----------------------------
pm_summary <- combined_data %>%
  group_by(city, area_type) %>%
  summarise(
    avg_pm2_5 = mean(pm2_5, na.rm = TRUE),   # average PM2.5
    avg_pm10 = mean(pm10, na.rm = TRUE),     # average PM10
    .groups = "drop"
  )

# ----------------------------
# Reshape the data so both PM2.5 and PM10 can be plotted together
# ----------------------------
# This turns two columns (avg_pm2_5 and avg_pm10) into one column of values
pm_long <- pm_summary %>%
  pivot_longer(
    cols = c(avg_pm2_5, avg_pm10),
    names_to = "pollutant",      # this new column will say "avg_pm2_5" or "avg_pm10"
    values_to = "value"          # this will contain the numeric values
  )

# ----------------------------
# Create bar plot to compare pollution levels by area type for each city
# ----------------------------
ggplot(pm_long, aes(x = city, y = value, fill = area_type)) +
  geom_bar(stat = "identity", position = "dodge") +  # make side-by-side bars
  facet_wrap(~ pollutant, scales = "free_y") +       # separate plot for PM2.5 and PM10
  labs(
    title = "Comparison of PM2.5 and PM10 by City and Area Type",
    x = "City",
    y = "Average Value",
    fill = "Area Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # rotate city names for readability

```

```{r}
# Step 11: Is there any correlation between the aqi,PM 2.5, PM 10, no2, O3, temperature and humidity in our dataset, show it using a correlation heatmap.
# Load required packages
library(ggplot2)
library(reshape2)   # for melting the correlation matrix
library(corrplot)   # for heatmap (optional alternative)

# ----------------------------
# Select only relevant numeric columns
# ----------------------------
# We'll only include the variables mentioned in the question
corr_data <- combined_data %>%
  select(aqi, pm2_5, pm10, no2, o3, temperature, humidity)

# ----------------------------
# Calculate correlation matrix
# ----------------------------
# This shows how strongly each variable is related to the others
cor_matrix <- cor(corr_data, use = "complete.obs")

# ----------------------------
# Melt the matrix into long format for plotting with ggplot
# ----------------------------
cor_long <- melt(cor_matrix)

# ----------------------------
# Plot heatmap using ggplot2
# ----------------------------
ggplot(cor_long, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +  # add white grid lines
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name = "Correlation") +
  labs(title = "Correlation Heatmap of Air Quality & Environmental Variables",
       x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))


```



